{
    "models": {
        "llama3-8b": {
            "path": "meta-llama/Meta-Llama-3-8B-Instruct",
            "use_chat_template": true,
            "use_system_prompt": true,
            "num_hidden_layers": 32
        },
        "llama3.1-8b": {
            "path": "meta-llama/Llama-3.1-8B-Instruct",
            "use_chat_template": true,
            "use_system_prompt": true,
            "num_hidden_layers": 32
        },
        "llama3.2-1b": {
            "path": "meta-llama/Llama-3.2-1B-Instruct",
            "use_chat_template": true,
            "use_system_prompt": true,
            "num_hidden_layers": 16
        },
        "llama3-70b": {
            "path": "meta-llama/Meta-Llama-3-70B-Instruct",
            "use_chat_template": true,
            "use_system_prompt": true,
            "num_hidden_layers": 80
        },
        "llama3.1-70b": {
            "path": "meta-llama/Llama-3.1-70B-Instruct",
            "use_chat_template": true,
            "use_system_prompt": true,
            "num_hidden_layers": 80
        },
        "llama3.3-70b": {
            "path": "meta-llama/Llama-3.3-70B-Instruct",
            "use_chat_template": true,
            "use_system_prompt": true,
            "num_hidden_layers": 80
        },
        "llama3-8b-base": {
            "path": "meta-llama/Meta-Llama-3-8B",
            "use_chat_template": false,
            "use_system_prompt": false,
            "num_hidden_layers": 32
        },
        "llama2-7b": {
            "path": "meta-llama/Llama-2-7b-chat-hf",
            "use_chat_template": true,
            "use_system_prompt": true
        },
        "qwen2-1.5b": {
            "path": "Qwen/Qwen2-1.5B-Instruct",
            "use_chat_template": true,
            "use_system_prompt": true,
            "num_hidden_layers": 28
        },
        "gemma-2-9b": {
            "path": "google/gemma-2-9b-it",
            "use_chat_template": true,
            "use_system_prompt": false,
            "num_hidden_layers": 42
        },
        "gemma-7b": {
            "path": "google/gemma-7b-it",
            "use_chat_template": true,
            "use_system_prompt": false,
            "num_hidden_layers": 28
        },
        "gemma-2-2b": {
            "path": "google/gemma-2-2b-it",
            "use_chat_template": true,
            "use_system_prompt": false,
            "num_hidden_layers": 26
        },
        "qwen2-7b": {
            "path": "Qwen/Qwen2-7B-Instruct",
            "use_chat_template": true,
            "use_system_prompt": true,
            "num_hidden_layers": 28
        },
        "mistral-7b": {
            "path": "mistralai/Mistral-7B-Instruct-v0.2",
            "use_chat_template": true,
            "use_system_prompt": true,
            "num_hidden_layers": 32
        },
        "gpt-120b": {
            "path": "openai/gpt-oss-120b",
            "use_chat_template": true,
            "use_system_prompt": true,
            "num_hidden_layers": 36,
            "dtype": "bfloat16"
        }
    }
}
